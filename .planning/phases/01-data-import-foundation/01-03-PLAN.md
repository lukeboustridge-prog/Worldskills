---
phase: 01-data-import-foundation
plan: 03
type: execute
wave: 3
depends_on: ["01-02"]
files_modified:
  - src/lib/import/text-normalizer.ts
  - src/lib/import/excel-parser.ts
  - src/lib/import/validator.ts
autonomous: true

must_haves:
  truths:
    - "Text normalizer converts smart quotes, bullets, and Unicode artifacts to clean text"
    - "Excel parser extracts descriptor data with complete performance levels"
    - "Merged cells are handled correctly"
    - "Validator catches encoding issues and data quality problems"
  artifacts:
    - path: "src/lib/import/text-normalizer.ts"
      provides: "Unicode normalization and smart quote conversion"
      exports: ["normalizeDescriptorText", "detectEncodingIssues"]
    - path: "src/lib/import/excel-parser.ts"
      provides: "ExcelJS-based marking scheme parser"
      exports: ["parseMarkingScheme", "ParsedDescriptor"]
    - path: "src/lib/import/validator.ts"
      provides: "Zod schemas for descriptor validation"
      exports: ["descriptorSchema", "validateDescriptor"]
  key_links:
    - from: "src/lib/import/excel-parser.ts"
      to: "src/lib/import/text-normalizer.ts"
      via: "import"
      pattern: "normalizeDescriptorText"
    - from: "src/lib/import/excel-parser.ts"
      to: "exceljs"
      via: "import"
      pattern: "ExcelJS"
---

<objective>
Create Excel parser with text normalization and validation for extracting descriptors from WSC2024 marking schemes.

Purpose: Build robust parsing logic that handles the variance discovered in the survey, normalizes text for consistent storage, and validates data quality before import.
Output: Parser, normalizer, and validator modules ready for bulk import.
</objective>

<execution_context>
@C:\Users\LukeBoustridge\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\LukeBoustridge\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-import-foundation/01-RESEARCH.md
@.planning/phases/01-data-import-foundation/01-02-SUMMARY.md
@survey-results.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create text normalizer</name>
  <files>src/lib/import/text-normalizer.ts</files>
  <action>
Create `src/lib/import/text-normalizer.ts`:

```typescript
/**
 * Text normalization for descriptor content extracted from Excel files.
 * Ensures consistent storage and searchability.
 */

/**
 * Normalizes text extracted from Excel files.
 * Handles: smart quotes, Unicode bullets, normalization form, whitespace.
 */
export function normalizeDescriptorText(text: string | null | undefined): string {
  if (!text) return '';

  // Step 1: Normalize to Unicode NFC (composed form)
  // Ensures "e" is stored as single character, not "e" + combining accent
  let normalized = String(text).normalize('NFC');

  // Step 2: Convert smart quotes to straight quotes
  normalized = normalized
    .replace(/[\u2018\u2019\u201A\u201B]/g, "'")  // ' ' ‚ ‛ → '
    .replace(/[\u201C\u201D\u201E\u201F]/g, '"')  // " " „ ‟ → "
    .replace(/[\u2032\u2035]/g, "'")               // ′ ‵ → '
    .replace(/[\u2033\u2036]/g, '"');              // ″ ‶ → "

  // Step 3: Convert Unicode bullets and list markers to hyphens
  normalized = normalized
    .replace(/[\u2022\u2023\u2043\u204C\u204D]/g, '-')  // • ‣ ⁃ ⁌ ⁍ → -
    .replace(/[\u25E6\u25AA\u25AB\u25CF\u25CB]/g, '-')  // ◦ ▪ ▫ ● ○ → -
    .replace(/[\u2219\u00B7]/g, '-');                    // ∙ · → -

  // Step 4: Convert non-breaking spaces to regular spaces
  normalized = normalized
    .replace(/[\u00A0\u202F\u2007\u200A]/g, ' ')  // NBSP, NNBSP, figure space, hair space
    .replace(/[\u2000-\u200B]/g, ' ');            // Various Unicode spaces

  // Step 5: Convert dashes/hyphens to standard hyphen
  normalized = normalized
    .replace(/[\u2010\u2011\u2012\u2013\u2014\u2015]/g, '-'); // ‐ ‑ ‒ – — ― → -

  // Step 6: Remove control characters (except newlines and tabs)
  normalized = normalized.replace(/[\x00-\x08\x0B\x0C\x0E-\x1F\x7F]/g, '');

  // Step 7: Normalize whitespace
  normalized = normalized
    .replace(/\r\n/g, '\n')           // Windows line endings
    .replace(/\r/g, '\n')             // Old Mac line endings
    .replace(/[ \t]+/g, ' ')          // Collapse horizontal whitespace
    .replace(/\n{3,}/g, '\n\n')       // Max 2 consecutive newlines
    .trim();

  return normalized;
}

/**
 * Detects potential encoding issues in text.
 * Returns warnings if suspicious patterns found.
 */
export function detectEncodingIssues(text: string): string[] {
  if (!text) return [];

  const warnings: string[] = [];

  // Check for mojibake patterns (common UTF-8 misinterpretation)
  if (/Ã©|Ã¨|Ã |Ã¢|Ã®|Ã´|Ã¹|Ã»|Ã§/i.test(text)) {
    warnings.push('Possible UTF-8 mojibake detected (French accents)');
  }
  if (/â€™|â€œ|â€|â€¢|â€"|â€"/i.test(text)) {
    warnings.push('Possible UTF-8 mojibake detected (smart quotes/dashes)');
  }

  // Check for replacement characters (encoding failure)
  if (/\uFFFD/.test(text)) {
    warnings.push('Unicode replacement character found (encoding failure)');
  }

  // Check for unusual control characters
  if (/[\x00-\x08\x0B\x0C\x0E-\x1F]/.test(text)) {
    warnings.push('Control characters detected (may indicate encoding issues)');
  }

  // Check for private use area characters
  if (/[\uE000-\uF8FF]/.test(text)) {
    warnings.push('Private use area characters found (font-specific symbols)');
  }

  // Check for very long strings without spaces (possible encoding issue)
  if (text.length > 500 && !/\s/.test(text)) {
    warnings.push('Very long string without whitespace (possible encoding issue)');
  }

  return warnings;
}

/**
 * Extracts skill name from filename.
 * "01_Industrial_Mechanics_marking_scheme (f).xlsx" → "Industrial Mechanics"
 */
export function extractSkillNameFromFilename(filename: string): string {
  return filename
    .replace(/\.xlsx$/i, '')                    // Remove extension
    .replace(/^\d+[_\-\s]*/g, '')               // Remove leading number
    .replace(/_marking_scheme.*$/i, '')         // Remove "marking_scheme" suffix
    .replace(/_/g, ' ')                         // Underscores to spaces
    .replace(/\s+/g, ' ')                       // Collapse whitespace
    .trim();
}
```

This handles:
- Unicode normalization (NFC form)
- Smart quotes (curly to straight)
- Unicode bullets (to hyphens)
- Non-breaking spaces
- Various dash types
- Control characters
- Whitespace normalization
- Encoding issue detection
  </action>
  <verify>Create a simple test in the script or verify via TypeScript compilation `npx tsc --noEmit`</verify>
  <done>Text normalizer handles smart quotes, bullets, Unicode artifacts, and detects encoding issues</done>
</task>

<task type="auto">
  <name>Task 2: Create Excel parser with survey-based configuration</name>
  <files>src/lib/import/excel-parser.ts</files>
  <action>
Create `src/lib/import/excel-parser.ts`. IMPORTANT: Reference `survey-results.json` to understand actual column structures found in the files.

```typescript
import ExcelJS from 'exceljs';
import { normalizeDescriptorText, detectEncodingIssues, extractSkillNameFromFilename } from './text-normalizer';

export interface ParsedDescriptor {
  code: string;
  criterionName: string;
  excellent: string;
  good: string;
  pass: string;
  belowPass: string;
  category?: string;
  skillName: string;
  warnings: string[];
}

export interface ParserConfig {
  sheetName?: string;
  headerRow: number;
  columns: {
    code: number | string[];      // Column index or possible header names
    criterionName: number | string[];
    excellent: number | string[];
    good: number | string[];
    pass: number | string[];
    belowPass: number | string[];
    category?: number | string[];
  };
}

// Default config based on survey results - adjust after running survey
const DEFAULT_COLUMN_PATTERNS = {
  code: ['code', 'id', 'ref', 'no', 'number', 'aspect'],
  criterionName: ['criterion', 'criteria', 'descriptor', 'description', 'aspect', 'sub-aspect', 'sub aspect'],
  excellent: ['excellent', 'exc', 'outstanding', '4', 'four'],
  good: ['good', 'satisfactory', '3', 'three'],
  pass: ['pass', 'acceptable', 'adequate', '2', 'two', 'sufficient'],
  belowPass: ['below', 'poor', 'unsatisfactory', 'fail', '1', 'one', 'insufficient', 'below pass']
};

/**
 * Finds column index by matching header names against patterns.
 */
function findColumnIndex(
  columns: { name: string; index: number }[],
  patterns: string[]
): number | undefined {
  for (const col of columns) {
    const name = col.name.toLowerCase().trim();
    for (const pattern of patterns) {
      if (name.includes(pattern.toLowerCase())) {
        return col.index;
      }
    }
  }
  return undefined;
}

/**
 * Gets cell text value, handling merged cells correctly.
 */
function getCellText(row: ExcelJS.Row, colNumber: number, sheet: ExcelJS.Worksheet): string {
  const cell = row.getCell(colNumber);

  // Handle merged cells - get master cell value
  if (cell.isMerged && cell.master) {
    return cell.master.text || '';
  }

  // Handle different cell value types
  if (cell.value === null || cell.value === undefined) {
    return '';
  }

  // Rich text
  if (typeof cell.value === 'object' && 'richText' in cell.value) {
    return (cell.value.richText as { text: string }[]).map(r => r.text).join('');
  }

  return cell.text || String(cell.value);
}

/**
 * Detects header row and extracts column information.
 */
function detectHeaderAndColumns(sheet: ExcelJS.Worksheet): {
  headerRow: number;
  columns: { name: string; index: number }[];
} {
  let headerRow = 1;
  const columns: { name: string; index: number }[] = [];

  // Find header row (first row with multiple non-empty cells that look like headers)
  for (let i = 1; i <= Math.min(20, sheet.rowCount); i++) {
    const row = sheet.getRow(i);
    const cells: { name: string; index: number }[] = [];

    row.eachCell({ includeEmpty: false }, (cell, colNumber) => {
      const text = getCellText(row, colNumber, sheet).toLowerCase();
      cells.push({ name: getCellText(row, colNumber, sheet), index: colNumber });
    });

    // Check if this looks like a header row (has multiple cells, contains descriptor-related terms)
    if (cells.length >= 3) {
      const headerTerms = ['criterion', 'descriptor', 'excellent', 'good', 'pass', 'code', 'aspect'];
      const hasHeaderTerms = cells.some(c =>
        headerTerms.some(term => c.name.toLowerCase().includes(term))
      );

      if (hasHeaderTerms) {
        headerRow = i;
        columns.push(...cells);
        break;
      }
    }
  }

  // Fallback: use first row with 3+ cells
  if (columns.length === 0) {
    for (let i = 1; i <= Math.min(10, sheet.rowCount); i++) {
      const row = sheet.getRow(i);
      const cells: { name: string; index: number }[] = [];

      row.eachCell({ includeEmpty: false }, (cell, colNumber) => {
        cells.push({ name: getCellText(row, colNumber, sheet), index: colNumber });
      });

      if (cells.length >= 3) {
        headerRow = i;
        columns.push(...cells);
        break;
      }
    }
  }

  return { headerRow, columns };
}

/**
 * Parses a single marking scheme file.
 */
export async function parseMarkingScheme(filePath: string): Promise<{
  descriptors: ParsedDescriptor[];
  errors: string[];
  warnings: string[];
}> {
  const errors: string[] = [];
  const warnings: string[] = [];
  const descriptors: ParsedDescriptor[] = [];

  try {
    const workbook = new ExcelJS.Workbook();
    await workbook.xlsx.readFile(filePath);

    // Find the appropriate sheet
    let sheet = workbook.getWorksheet('Marking Scheme')
      || workbook.getWorksheet('MS')
      || workbook.worksheets.find(ws =>
          ws.name.toLowerCase().includes('mark') ||
          ws.name.toLowerCase().includes('criteria')
        )
      || workbook.worksheets[0];

    if (!sheet) {
      errors.push('No worksheet found');
      return { descriptors, errors, warnings };
    }

    // Detect header and columns
    const { headerRow, columns } = detectHeaderAndColumns(sheet);

    if (columns.length === 0) {
      errors.push('Could not detect header row');
      return { descriptors, errors, warnings };
    }

    // Map columns to descriptor fields
    const colMapping = {
      code: findColumnIndex(columns, DEFAULT_COLUMN_PATTERNS.code),
      criterionName: findColumnIndex(columns, DEFAULT_COLUMN_PATTERNS.criterionName),
      excellent: findColumnIndex(columns, DEFAULT_COLUMN_PATTERNS.excellent),
      good: findColumnIndex(columns, DEFAULT_COLUMN_PATTERNS.good),
      pass: findColumnIndex(columns, DEFAULT_COLUMN_PATTERNS.pass),
      belowPass: findColumnIndex(columns, DEFAULT_COLUMN_PATTERNS.belowPass),
      category: findColumnIndex(columns, ['category', 'section', 'module', 'area'])
    };

    // Validate required columns found
    if (!colMapping.criterionName && !colMapping.code) {
      errors.push(`Could not find criterion/code columns. Found: ${columns.map(c => c.name).join(', ')}`);
      return { descriptors, errors, warnings };
    }

    // Extract skill name from filename
    const filename = filePath.split(/[/\\]/).pop() || '';
    const skillName = extractSkillNameFromFilename(filename);

    // Track current category (for merged cells)
    let currentCategory = '';

    // Parse data rows
    sheet.eachRow((row, rowNumber) => {
      // Skip header and empty rows
      if (rowNumber <= headerRow) return;

      // Check for category row (often in merged cell spanning columns)
      if (colMapping.category) {
        const categoryText = getCellText(row, colMapping.category, sheet);
        if (categoryText && categoryText.length > 0 && categoryText.length < 100) {
          currentCategory = normalizeDescriptorText(categoryText);
        }
      }

      // Get criterion name (required)
      const criterionCol = colMapping.criterionName || colMapping.code;
      if (!criterionCol) return;

      const criterionName = normalizeDescriptorText(getCellText(row, criterionCol, sheet));
      if (!criterionName || criterionName.length < 2) return;

      // Get code (use row number if not available)
      let code = colMapping.code
        ? normalizeDescriptorText(getCellText(row, colMapping.code, sheet))
        : '';

      if (!code) {
        code = `R${rowNumber}`;
      }

      // Get performance levels
      const excellent = colMapping.excellent
        ? normalizeDescriptorText(getCellText(row, colMapping.excellent, sheet))
        : '';
      const good = colMapping.good
        ? normalizeDescriptorText(getCellText(row, colMapping.good, sheet))
        : '';
      const pass = colMapping.pass
        ? normalizeDescriptorText(getCellText(row, colMapping.pass, sheet))
        : '';
      const belowPass = colMapping.belowPass
        ? normalizeDescriptorText(getCellText(row, colMapping.belowPass, sheet))
        : '';

      // Collect warnings
      const rowWarnings: string[] = [];
      [criterionName, excellent, good, pass, belowPass].forEach(text => {
        rowWarnings.push(...detectEncodingIssues(text));
      });

      // Only add if we have meaningful content
      if (criterionName.length >= 5 || excellent || good || pass || belowPass) {
        descriptors.push({
          code,
          criterionName,
          excellent,
          good,
          pass,
          belowPass,
          category: currentCategory || undefined,
          skillName,
          warnings: rowWarnings
        });
      }
    });

    if (descriptors.length === 0) {
      warnings.push('No descriptors extracted from file');
    }

  } catch (error) {
    errors.push(error instanceof Error ? error.message : String(error));
  }

  return { descriptors, errors, warnings };
}
```

Key features:
- Auto-detects header row based on content
- Maps columns by matching patterns (flexible for variance)
- Handles merged cells for categories
- Applies text normalization immediately
- Tracks encoding warnings
- Generates fallback codes if missing
  </action>
  <verify>Run `npx tsc --noEmit` to verify TypeScript compilation</verify>
  <done>Excel parser extracts descriptors with performance levels, handles merged cells, and normalizes text</done>
</task>

<task type="auto">
  <name>Task 3: Create validator with Zod schemas</name>
  <files>src/lib/import/validator.ts</files>
  <action>
Create `src/lib/import/validator.ts`:

```typescript
import { z } from 'zod';
import type { ParsedDescriptor } from './excel-parser';

/**
 * Schema for a parsed descriptor before database insertion.
 */
export const parsedDescriptorSchema = z.object({
  code: z.string().min(1, 'Code is required'),
  criterionName: z.string().min(2, 'Criterion name must be at least 2 characters'),
  excellent: z.string().default(''),
  good: z.string().default(''),
  pass: z.string().default(''),
  belowPass: z.string().default(''),
  category: z.string().optional(),
  skillName: z.string().min(1, 'Skill name is required'),
  warnings: z.array(z.string()).default([])
});

/**
 * Schema for descriptor ready for database insertion.
 */
export const descriptorImportSchema = z.object({
  code: z.string().min(1),
  criterionName: z.string().min(2),
  excellent: z.string().nullable(),
  good: z.string().nullable(),
  pass: z.string().nullable(),
  belowPass: z.string().nullable(),
  category: z.string().nullable(),
  skillName: z.string().min(1),
  sector: z.string().nullable(),
  source: z.literal('WSC2024'),
  version: z.number().int().positive().default(1),
  tags: z.array(z.string()).default([])
});

export type ParsedDescriptorInput = z.input<typeof parsedDescriptorSchema>;
export type DescriptorImport = z.infer<typeof descriptorImportSchema>;

export interface ValidationResult {
  valid: boolean;
  data?: DescriptorImport;
  errors: string[];
  warnings: string[];
}

/**
 * Validates a parsed descriptor and prepares it for database insertion.
 */
export function validateDescriptor(descriptor: ParsedDescriptor): ValidationResult {
  const errors: string[] = [];
  const warnings: string[] = [...descriptor.warnings];

  // Basic validation
  if (!descriptor.code || descriptor.code.trim().length === 0) {
    errors.push('Missing code');
  }

  if (!descriptor.criterionName || descriptor.criterionName.trim().length < 2) {
    errors.push('Criterion name too short or missing');
  }

  if (!descriptor.skillName || descriptor.skillName.trim().length === 0) {
    errors.push('Missing skill name');
  }

  // Check for minimum content (at least criterion name OR one performance level)
  const hasContent = descriptor.criterionName.length >= 5 ||
    descriptor.excellent.length > 0 ||
    descriptor.good.length > 0 ||
    descriptor.pass.length > 0 ||
    descriptor.belowPass.length > 0;

  if (!hasContent) {
    errors.push('Descriptor has no meaningful content');
  }

  // Quality warnings
  if (descriptor.excellent.length === 0 && descriptor.good.length === 0 &&
      descriptor.pass.length === 0 && descriptor.belowPass.length === 0) {
    warnings.push('No performance levels defined');
  }

  if (descriptor.criterionName.length > 500) {
    warnings.push('Criterion name is unusually long (>500 chars)');
  }

  const allLevels = [descriptor.excellent, descriptor.good, descriptor.pass, descriptor.belowPass];
  if (allLevels.some(l => l.length > 2000)) {
    warnings.push('Performance level text is unusually long (>2000 chars)');
  }

  if (errors.length > 0) {
    return { valid: false, errors, warnings };
  }

  // Prepare for database
  const data: DescriptorImport = {
    code: descriptor.code.trim(),
    criterionName: descriptor.criterionName.trim(),
    excellent: descriptor.excellent || null,
    good: descriptor.good || null,
    pass: descriptor.pass || null,
    belowPass: descriptor.belowPass || null,
    category: descriptor.category || null,
    skillName: descriptor.skillName.trim(),
    sector: null, // Could be enriched from skill metadata
    source: 'WSC2024',
    version: 1,
    tags: []
  };

  return { valid: true, data, errors: [], warnings };
}

/**
 * Validates a batch of descriptors.
 */
export function validateDescriptorBatch(descriptors: ParsedDescriptor[]): {
  valid: DescriptorImport[];
  invalid: { descriptor: ParsedDescriptor; errors: string[] }[];
  allWarnings: { descriptor: ParsedDescriptor; warnings: string[] }[];
} {
  const valid: DescriptorImport[] = [];
  const invalid: { descriptor: ParsedDescriptor; errors: string[] }[] = [];
  const allWarnings: { descriptor: ParsedDescriptor; warnings: string[] }[] = [];

  for (const descriptor of descriptors) {
    const result = validateDescriptor(descriptor);

    if (result.valid && result.data) {
      valid.push(result.data);
    } else {
      invalid.push({ descriptor, errors: result.errors });
    }

    if (result.warnings.length > 0) {
      allWarnings.push({ descriptor, warnings: result.warnings });
    }
  }

  return { valid, invalid, allWarnings };
}
```

Validation rules:
- Required: code, criterionName, skillName
- Must have meaningful content (criterion name >=5 chars OR at least one performance level)
- Warns on missing performance levels
- Warns on unusually long content
- Prepares data structure for Prisma insertion
  </action>
  <verify>Run `npx tsc --noEmit` to verify TypeScript compilation</verify>
  <done>Validator catches encoding issues, validates required fields, and prepares data for import</done>
</task>

</tasks>

<verification>
- All files compile: `npx tsc --noEmit` passes
- Text normalizer handles smart quotes: `normalizeDescriptorText("Hello 'world'")` returns `"Hello 'world'"`
- Parser can read a sample Excel file without errors
- Validator rejects descriptors without required fields
</verification>

<success_criteria>
1. Text normalizer converts smart quotes, bullets, Unicode artifacts to clean text
2. Excel parser auto-detects header rows and column mappings
3. Parser handles merged cells correctly (extracts master cell value)
4. Validator validates required fields and detects quality issues
5. All modules export their interfaces for use by importer
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-import-foundation/01-03-SUMMARY.md`

Include in summary:
- Normalizer transformations implemented
- Parser column detection approach
- Validation rules applied
</output>
