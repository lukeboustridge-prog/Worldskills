---
phase: 01-data-import-foundation
plan: 04
type: execute
wave: 4
depends_on: ["01-03"]
files_modified:
  - src/lib/import/importer.ts
  - src/scripts/import-wsc2024-descriptors.ts
  - import-report.json
autonomous: true

must_haves:
  truths:
    - "All 58 WSC2024 files are processed"
    - "Descriptors are imported with transaction safety (batch rollback on failure)"
    - "Import failures are logged with enough detail for manual review"
    - "Source skill metadata captured for every descriptor"
  artifacts:
    - path: "src/lib/import/importer.ts"
      provides: "Batched transaction import with error recovery"
      exports: ["importDescriptors", "ImportResult"]
    - path: "src/scripts/import-wsc2024-descriptors.ts"
      provides: "Executable import script"
      contains: "importDescriptors"
    - path: "import-report.json"
      provides: "Complete import log with success/failure counts"
  key_links:
    - from: "src/scripts/import-wsc2024-descriptors.ts"
      to: "src/lib/import/importer.ts"
      via: "import"
      pattern: "importDescriptors"
    - from: "src/lib/import/importer.ts"
      to: "prisma"
      via: "$transaction"
      pattern: "prisma.\\$transaction"
---

<objective>
Create bulk importer with batched transactions and execute WSC2024 descriptor import.

Purpose: Import all descriptors from 58 files with transaction safety, proper error handling, and detailed logging for audit trail.
Output: All WSC2024 descriptors imported to database, import report generated.
</objective>

<execution_context>
@C:\Users\LukeBoustridge\.claude/get-shit-done/workflows/execute-plan.md
@C:\Users\LukeBoustridge\.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/01-data-import-foundation/01-RESEARCH.md
@.planning/phases/01-data-import-foundation/01-03-SUMMARY.md
@survey-results.json
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create batched transaction importer</name>
  <files>src/lib/import/importer.ts</files>
  <action>
Create `src/lib/import/importer.ts`:

```typescript
import { prisma } from '@/lib/prisma';
import type { DescriptorImport } from './validator';

const BATCH_SIZE = 500; // Avoid long-running transactions

export interface ImportResult {
  totalProcessed: number;
  successCount: number;
  failedCount: number;
  duplicateCount: number;
  batches: BatchResult[];
}

export interface BatchResult {
  batchNumber: number;
  recordCount: number;
  success: boolean;
  insertedCount: number;
  error?: string;
  duration: number;
}

/**
 * Splits array into chunks of specified size.
 */
function chunk<T>(array: T[], size: number): T[][] {
  const chunks: T[][] = [];
  for (let i = 0; i < array.length; i += size) {
    chunks.push(array.slice(i, i + size));
  }
  return chunks;
}

/**
 * Imports descriptors in batched transactions.
 * Each batch is atomic - if a batch fails, only that batch rolls back.
 */
export async function importDescriptors(
  descriptors: DescriptorImport[],
  options: { continueOnError?: boolean } = {}
): Promise<ImportResult> {
  const { continueOnError = true } = options;

  const batches = chunk(descriptors, BATCH_SIZE);
  const batchResults: BatchResult[] = [];

  let totalSuccess = 0;
  let totalFailed = 0;
  let totalDuplicates = 0;

  console.log(`\nImporting ${descriptors.length} descriptors in ${batches.length} batches...`);

  for (let i = 0; i < batches.length; i++) {
    const batch = batches[i];
    const startTime = Date.now();

    try {
      // Use transaction for atomic batch insert
      const result = await prisma.$transaction(async (tx) => {
        // createMany with skipDuplicates handles composite unique constraint
        const created = await tx.descriptor.createMany({
          data: batch.map(d => ({
            code: d.code,
            criterionName: d.criterionName,
            excellent: d.excellent,
            good: d.good,
            pass: d.pass,
            belowPass: d.belowPass,
            category: d.category,
            skillName: d.skillName,
            sector: d.sector,
            source: d.source,
            version: d.version,
            tags: d.tags
          })),
          skipDuplicates: true // Skip [skillName, code] duplicates
        });

        return created;
      });

      const duration = Date.now() - startTime;
      const duplicatesInBatch = batch.length - result.count;

      totalSuccess += result.count;
      totalDuplicates += duplicatesInBatch;

      batchResults.push({
        batchNumber: i + 1,
        recordCount: batch.length,
        success: true,
        insertedCount: result.count,
        duration
      });

      console.log(`  Batch ${i + 1}/${batches.length}: ✓ ${result.count} inserted (${duplicatesInBatch} duplicates) [${duration}ms]`);

    } catch (error) {
      const duration = Date.now() - startTime;
      const errorMessage = error instanceof Error ? error.message : String(error);

      totalFailed += batch.length;

      batchResults.push({
        batchNumber: i + 1,
        recordCount: batch.length,
        success: false,
        insertedCount: 0,
        error: errorMessage,
        duration
      });

      console.error(`  Batch ${i + 1}/${batches.length}: ✗ FAILED [${duration}ms]`);
      console.error(`    Error: ${errorMessage}`);

      if (!continueOnError) {
        throw new Error(`Import aborted at batch ${i + 1}: ${errorMessage}`);
      }
    }
  }

  return {
    totalProcessed: descriptors.length,
    successCount: totalSuccess,
    failedCount: totalFailed,
    duplicateCount: totalDuplicates,
    batches: batchResults
  };
}

/**
 * Counts existing descriptors by source.
 */
export async function countDescriptors(source?: string): Promise<number> {
  return prisma.descriptor.count({
    where: source ? { source } : undefined
  });
}

/**
 * Deletes all descriptors with a specific source (for reimport).
 */
export async function deleteDescriptorsBySource(source: string): Promise<number> {
  const result = await prisma.descriptor.deleteMany({
    where: { source }
  });
  return result.count;
}
```

Key features:
- Batches of 500 records (avoids transaction timeout)
- Each batch is atomic (rolls back on failure)
- Continues importing other batches if one fails
- Uses skipDuplicates for composite unique constraint
- Tracks duplicates separately from failures
- Detailed timing and logging
  </action>
  <verify>Run `npx tsc --noEmit` to verify TypeScript compilation</verify>
  <done>Batched importer with transaction safety, duplicate handling, and error recovery</done>
</task>

<task type="auto">
  <name>Task 2: Create and execute import script</name>
  <files>src/scripts/import-wsc2024-descriptors.ts, import-report.json</files>
  <action>
Create `src/scripts/import-wsc2024-descriptors.ts`:

```typescript
import fs from 'fs/promises';
import path from 'path';
import { parseMarkingScheme } from '@/lib/import/excel-parser';
import { validateDescriptorBatch, type DescriptorImport } from '@/lib/import/validator';
import { importDescriptors, countDescriptors, type ImportResult } from '@/lib/import/importer';

const SOURCE_DIR = 'C:\\Users\\LukeBoustridge\\Dropbox\\WSI standards and assessment\\WSC2024\\Skill Advisors\\Final MS by Skill';

interface FileParseResult {
  fileName: string;
  descriptorCount: number;
  validCount: number;
  invalidCount: number;
  warnings: number;
  errors: string[];
}

interface ImportReport {
  startTime: string;
  endTime: string;
  duration: string;
  sourceDirectory: string;
  fileResults: FileParseResult[];
  totalDescriptors: number;
  validDescriptors: number;
  invalidDescriptors: number;
  importResult: ImportResult | null;
  databaseCount: number;
}

async function runImport(): Promise<void> {
  const startTime = new Date();
  console.log('=== WSC2024 Descriptor Import ===');
  console.log(`Start time: ${startTime.toISOString()}`);
  console.log(`Source: ${SOURCE_DIR}\n`);

  // Get existing count
  const existingCount = await countDescriptors('WSC2024');
  if (existingCount > 0) {
    console.log(`⚠️  Found ${existingCount} existing WSC2024 descriptors.`);
    console.log('   New descriptors with duplicate [skillName, code] will be skipped.\n');
  }

  // Read all Excel files
  const files = await fs.readdir(SOURCE_DIR);
  const xlsxFiles = files.filter(f => f.endsWith('.xlsx') && !f.startsWith('~'));

  console.log(`Found ${xlsxFiles.length} Excel files\n`);

  const fileResults: FileParseResult[] = [];
  const allDescriptors: DescriptorImport[] = [];

  // Parse all files
  console.log('--- PARSING PHASE ---');
  for (const file of xlsxFiles) {
    console.log(`  Parsing: ${file}`);

    const filePath = path.join(SOURCE_DIR, file);
    const { descriptors, errors, warnings } = await parseMarkingScheme(filePath);

    const validation = validateDescriptorBatch(descriptors);

    fileResults.push({
      fileName: file,
      descriptorCount: descriptors.length,
      validCount: validation.valid.length,
      invalidCount: validation.invalid.length,
      warnings: validation.allWarnings.length,
      errors
    });

    allDescriptors.push(...validation.valid);

    if (errors.length > 0) {
      console.log(`    ✗ Errors: ${errors.join(', ')}`);
    }
    if (validation.invalid.length > 0) {
      console.log(`    ⚠ Invalid: ${validation.invalid.length} descriptors skipped`);
    }
    console.log(`    ✓ Valid: ${validation.valid.length} descriptors`);
  }

  // Summary
  const totalParsed = fileResults.reduce((sum, f) => sum + f.descriptorCount, 0);
  const totalValid = allDescriptors.length;
  const totalInvalid = fileResults.reduce((sum, f) => sum + f.invalidCount, 0);

  console.log('\n--- PARSING SUMMARY ---');
  console.log(`Files processed: ${fileResults.length}`);
  console.log(`Total descriptors found: ${totalParsed}`);
  console.log(`Valid for import: ${totalValid}`);
  console.log(`Invalid (skipped): ${totalInvalid}`);

  // Import to database
  console.log('\n--- IMPORT PHASE ---');
  let importResult: ImportResult | null = null;

  if (allDescriptors.length > 0) {
    importResult = await importDescriptors(allDescriptors, { continueOnError: true });

    console.log('\n--- IMPORT SUMMARY ---');
    console.log(`Total processed: ${importResult.totalProcessed}`);
    console.log(`Successfully imported: ${importResult.successCount}`);
    console.log(`Duplicates skipped: ${importResult.duplicateCount}`);
    console.log(`Failed: ${importResult.failedCount}`);
  } else {
    console.log('No valid descriptors to import.');
  }

  // Final count
  const finalCount = await countDescriptors('WSC2024');

  const endTime = new Date();
  const duration = ((endTime.getTime() - startTime.getTime()) / 1000).toFixed(1);

  console.log('\n=== IMPORT COMPLETE ===');
  console.log(`Duration: ${duration} seconds`);
  console.log(`WSC2024 descriptors in database: ${finalCount}`);

  // Save report
  const report: ImportReport = {
    startTime: startTime.toISOString(),
    endTime: endTime.toISOString(),
    duration: `${duration}s`,
    sourceDirectory: SOURCE_DIR,
    fileResults,
    totalDescriptors: totalParsed,
    validDescriptors: totalValid,
    invalidDescriptors: totalInvalid,
    importResult,
    databaseCount: finalCount
  };

  const reportPath = path.join(process.cwd(), 'import-report.json');
  await fs.writeFile(reportPath, JSON.stringify(report, null, 2));
  console.log(`\nReport saved to: ${reportPath}`);
}

runImport().catch(error => {
  console.error('\n=== IMPORT FAILED ===');
  console.error(error);
  process.exit(1);
});
```

Then execute the import:

```bash
npx tsx src/scripts/import-wsc2024-descriptors.ts
```

This will:
1. Parse all 58 Excel files
2. Validate and normalize descriptors
3. Import in batches of 500
4. Generate detailed report

Monitor console output for progress and any errors.
  </action>
  <verify>
- import-report.json exists and shows success
- Query database: `npx prisma studio` and view Descriptor table
- Verify descriptor count matches expected (~12,000)
  </verify>
  <done>All WSC2024 descriptors imported, report shows success count, database contains imported data</done>
</task>

</tasks>

<verification>
- `import-report.json` exists with complete results
- No batch failures in import result
- `npx prisma studio` shows Descriptor table with data
- Running `SELECT COUNT(*) FROM "Descriptor" WHERE source = 'WSC2024'` returns thousands of rows
- Random sampling shows correct data (skill names, performance levels populated)
</verification>

<success_criteria>
1. All 58 files processed without fatal errors
2. Descriptors imported with batched transactions (500 per batch)
3. Import failures logged with batch details
4. Source metadata (skillName, source='WSC2024') captured for every descriptor
5. import-report.json contains complete audit trail
6. Database contains expected number of WSC2024 descriptors
</success_criteria>

<output>
After completion, create `.planning/phases/01-data-import-foundation/01-04-SUMMARY.md`

Include in summary:
- Total files processed
- Total descriptors imported
- Any files with errors (and what errors)
- Final database count
</output>
